the statistical analysis is meant to collect and study the information available in large quantities. Statistics is a branch of mathematics, where computation is done over a bulk of data using charts, tables, graphs, etc.



    Importance of Statistics:

Statistics executes the work simply and gives a transparent picture of the work we do regularly.
The statistical methods help us to examine different areas such as medicine, business, economics, social science and others.
Statistics equips us with different kinds of organised data with the help of graphs, tables, diagrams and charts.
Statistics helps to understand the variability of the data pattern in a quantitative way
Statistics makes us understand the bulk of data in a simple way


Basic terminology of Statistics :

    Population –

    It is actually a collection of set of individuals or objects or events whose properties are to be analyzed.

    Sample –

    It is the subset of a population.


Statistics have majorly categorised into two types:

    Descriptive statistics
    Inferential statistics


    Descriptive Statistics

In this type of statistics, the data is summarised through the given observations. The summarisation is one from a sample of population using parameters such as the mean or standard deviation.    
Descriptive statistics uses data that provides a description of the population either through numerical calculation or graph or table.

Descriptive statistics is a way to organise, represent and describe a collection of data using tables, graphs, and summary measures. 


There are two categories in this as following below:


(a). Measure of central tendency –
Measure of central tendency is also known as summary statistics that is used to represents the center point or a particular value of a data set or sample set.
In statistics, there are three common measures of central tendency as shown below:



Population Mean: 
Population mean represents the actual mean of the whole population.
Symbol: μ (Greek term mu)
Calculation: Difficult
Accuracy:	High
Standard deviation: When calculated using population mean, is denoted by (σ).


Sample Mean: 
Sample mean is the arithmetic mean of random sample values drawn from the population.	
Symbol:	x̄ (pronounced as x bar)
Calculation:	Easy	
Accuracy:	Low	
Standard deviation: When calculated using sample mean, is denoted by (s).


   
     (i) Mean :
    It is measure of average of all value in a sample set.
    For example        
        :picture on the picture folder

    (ii) Median :
    It is measure of central value of a sample set. In these, data set is ordered from lowest to highest value and then finds exact middle.
For example,        
        :picture on the picture folder

    (iii) Mode :
    It is value most frequently arrived in sample set. The value repeated most of time in central set is actually mode.
For example,        
        :picture on the picture folder


        
(b). Measure of Variability –
Measure of Variability is also known as measure of dispersion and used to describe variability in a sample or population. In statistics, there are three common measures of variability as shown below:
    (i) Range :
    It is given measure of how spread apart values in sample set or data set.
                Range = Maximum value - Minimum value 


    (ii) Variance :
    It simply describes how much a random variable defers from expected value and it is also computed as square of deviation.
                    S^2= ∑ni=1 [(xi - ͞x)^2 ÷ n]    
    In these formula, n represent total data points, ͞x represent mean of data points and xi represent individual data points.

    (iii) Dispersion :
    It is measure of dispersion of set of data from its mean.
                    σ= √ (1÷n) ∑ni=1 (xi - μ)2  


Measure Of Dispersion(Variance, Standard
Deviation):

Dispersion is the state of getting dispersed or spread. Statistical dispersion means the extent to which numerical data is likely to vary about an average value. In other words, dispersion helps to understand the distribution of the data.


Range: It is simply the difference between the maximum value and the minimum value given in a data set. Example: 1, 3,5, 6, 7 => Range = 7 -1= 6

Variance: The average squared deviation from the mean of the given data set is known as the variance. This measure of dispersion checks the spread of the data about the mean. Variance (σ^2) = ∑(X−μ)2/N


Standard Deviation: The square root of the variance gives the standard deviation. 
i.e. S.D. = √σ.
Thus, the standard deviation also measures the variation of the data about the mean. 





Class 10th Statistics:

Exclusive form/Continuous/Overlapping form: Higher limit of one class interval is equal to the lower limit of the next class. it is called exclusive form because upper limit in the given data is excluded from the each class interval. 
    e.g., 0-10 and 10-20 and 20-30

inclusive form/Discontinuous/Nonoverlapping : It does'nt overlaps. Upper limit is also included in the class itself. and so it is not included in the next class.
    e.g., 1-10 and 11-20 and 21-30

We have to focus on Continuous or exclusive form because every time we have to convert discontinuous or inclusive data into Continuous or Exclusive form.

How to convert : Add 0.5 on the upper limit and subtrack 0.5 on the lower limit.


Mean = Average. represented by x bar.


Cumulative Frequency(c.f): Cumulative Frequency of a class interval is determined by adding all the frequencies upto that class interval.
    









--------------------------------------------------------------


Inferential Statistics:
This type of statistics is used to interpret the meaning of Descriptive statistics. That means once the data has been collected, analysed and summarised then we use these stats to describe the meaning of the collected data. Or we can say, it is used to draw conclusions from the data that depends on random variations such as observational errors, sampling variation, etc.
It generalizes a large dataset and applies probabilities to draw a conclusion.

Inferential Statistics is a method that allows us to use information collected from a sample to make decisions, predictions or inferences from a population. 
Inferential Statistics is mainly related to and associated with hypothesis testing whose main target is to reject null hypothesis.
For example, deriving estimates from hypothetical research.


    Types of inferential statistics –
Various types of inferential statistics are used widely nowadays and are very easy to interpret. These are given below:

One sample test of difference/One sample hypothesis test
Confidence Interval
Contingency Tables and Chi-Square Statistic
T-test or Anova
Pearson Correlation
Bi-variate Regression
Multi-variate Regression












Variables:
-Quantitative vs categorical variables:

1. Quantitative or numerical variables:
When you collect quantitative data, the numbers you record represent real amounts that can be added, subtracted, divided, etc. There are two types of quantitative variables: discrete and continuous.


        a)Discrete variables (aka integer variables): Counts of individual items or values.  e.g., Number of students in a class, Number of different tree species in a forest.

        b)Continuous variables (aka ratio variables):  The value given to an observation for a continuous variable can include values as small as the instrument of measurement allows. Measurements of continuous or non-finite values. e.g., ..Distance, Volume, Age, height, time, and temperature, etc.



2. Categorical variables: Categorical variables have values that describe a 'quality' or 'characteristic' of a data unit, like 'what type' or 'which category'. Categorical variables fall into mutually exclusive (in one category or in another) and exhaustive (include all possible options) categories.
    e.g., gender: male, female.
        Brands, 


        a) ordinal variable: Observations can take a value that can be logically ordered or ranked. The categories associated with ordinal variables can be ranked higher or lower than another. e.g., academic grades (i.e. A, B, C), clothing size (i.e. small, medium, large, extra large) and attitudes (i.e. strongly agree, agree, disagree, strongly disagree).

        b) nominal variable:  Observations can take a value that is not able to be organised in a logical sequence. Examples of nominal categorical variables include sex, business type, eye colour, religion and brand.




##Levels of measurement:
Levels of measurement, also called scales of measurement, tell you how precisely variables are recorded. In scientific research, a variable is anything that can take on different values across your data set (e.g., height or test scores).

There are 4 levels of measurement:

    Nominal: the data can only be categorized

    Ordinal: the data can be categorized and ranked

    Interval: the data can be categorized, ranked, and evenly spaced

    Ratio: the data can be categorized, ranked, evenly spaced, and has a natural zero.

Depending on the level of measurement of the variable, what you can do to analyze your data may be limited. There is a hierarchy in the complexity and precision of the level of measurement, from low (nominal) to high (ratio).

Nominal, ordinal, interval, and ratio data
Going from lowest to highest, the 4 levels of measurement are cumulative. This means that they each take on the properties of lower levels and add new properties.

        a) Nominal level: You can categorize your data by labelling them in mutually exclusive groups, but there is no order between the categories. e.g., 
            City of birth
            Gender
            Ethnicity
            Car brands
            Marital status

        b) Ordinal level: You can categorize and rank your data in an order, but you cannot say anything about the intervals between the rankings.
Although you can rank the top 5 Olympic medallists, this scale does not tell you how close or far apart they are in number of wins.
            Top 5 Olympic medallists
            Language ability (e.g., beginner, intermediate, fluent)
            Likert-type questions  (e.g., very dissatisfied to very satisfied)

        c) Interval level: You can categorize, rank, and infer equal intervals between neighboring data points, but there is no true zero point.
The difference between any two adjacent temperatures is the same: one degree. But  zero degrees is defined differently depending on the scale – it doesn’t mean an absolute absence of temperature.

The same is true for test scores and personality inventories. A zero on a test is arbitrary; it does not mean that the test-taker has an absolute lack of the trait being measured. e.g., 

                Test scores (e.g., IQ or exams)
                Personality inventories
                Temperature in Fahrenheit or Celsius

        d) Ratio level	: You can categorize, rank, and infer equal intervals between neighboring data points, and there is a true zero point.
A true zero means there is an absence of the variable of interest. In ratio scales, zero does mean an absolute lack of the variable.

For example, in the Kelvin temperature scale, there are no negative degrees of temperature – zero means an absolute lack of thermal energy.

                Height
                Age
                Weight
                Temperature in Kelvin

Why are levels of measurement important?
The level at which you measure a variable determines how you can analyze your data.

The different levels limit which descriptive statistics you can use to get an overall summary of your data, and which type of inferential statistics you can perform on your data to support or refute your hypothesis.

In many cases, your variables can be measured at different levels, so you have to choose the level of measurement you will use before data collection begins.




                                 





    Quantiles and Percentiles: 
Quantile: The median is a quantile. because it splits the data into groups that contain the same number of data points. 
e.g., 50% Quantile. 25% Quantile. median is called the 0.5 quantile or 50% quantile.


Percentile: The percentiles are just quantiles that divide the data into 100 equally sized groups. 
We call the median as the 50% quantile or the 50th Percentile.
e.g., 25th percentile, 50th percentile.



    #Five Number Summary:

The five number summary includes 5 items:

The minimum.
Q1 (the first quartile, or the 25% mark).
The median.
Q3 (the third quartile, or the 75% mark).
The maximum.


# Inter Quartile Range(IQR):

The interquartile range is calculated in much the same way as the range. All you do to find it is subtract the first quartile from the third quartile:

IQR = Q3 – Q1.


Find Outliers: 

This is done using these steps:

1. Calculate the interquartile range for the data.
2. Multiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers).
3. Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.
4. Subtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier.






1. IQR = Q3 - Q1

2. Lower Bound: (Q1 - 1.5 * IQR)
3. Upper Bound: (Q3 + 1.5 * IQR)






            e.g., :
A survey was given to a random sample of 20 sophomore college students. They were asked, “how many textbooks do you own?” Their responses, were: 
        0, 0, 2, 5, 8, 8, 8, 9, 9, 10, 10, 10, 11, 12, 12, 12, 14, 15, 20, and 25.

The observations are in order from smallest to largest, we can now compute the IQR by finding the median followed by Q1 and Q3.

Median = 10
Q1 = 8
Q3 = 12
IQR = 12 - 8 = 4

The interquartile range is 4.

1.5 X IQR = 1.5 x (4) = 6

1.5 times the interquartile range is 6. Our fences will be 6 points below Q1 and 6 points above Q3.

Lower fence: 8 - 6 = 2
Upper fence: 12 + 6 = 18

Any observations less than 2 books or greater than 18 books are outliers. There are 4 outliers: 0, 0, 20, and 25.




OUTLIERS: 
Due to outliers, some of the values get drastically effected:
            Mean
            Range
            Standard Deviation.






Prabability Density Curve:
A density curve is just a curve that help us to visualize the ovarall shape of a distribution.

This means that density curves can represent measurements such as time and weight (which are continuous), and NOT situations such as rolling a die (which would be discrete). 

A density curve is a curve on a graph that represents the distribution of values in a dataset. It’s useful for three reasons:



1. A density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.

2. A density curve lets us visually see where the mean and the median of a distribution are located.

3. A density curve lets us visually see what percentage of observations in a dataset fall between different values.

The most famous density curve is the bell-shaped curve that represents the normal distribution.



How to Interpret Density Curves
Density curves come in all shapes and sizes and they allow us to gain a quick visual understanding of the distribution of values in a given dataset. In particular, they’re useful for helping us visualize:

1. Skewness

Skewness is a way to describe the symmetry of a distribution. Density curves allow us to quickly see whether or not a graph is left skewed, right skewed, or has no skew:


2. The location of the mean & median

Depending on the skewness of a density curve, we can quickly know whether the mean or median is larger in a given distribution. In particular:

If a density curve is left skewed, then the mean is less than the median.
If a density curve is right skewed, then the mean is greater than the median.
If a density curve has no skew, then the mean is equal to the median.

Properties of Density Curves
Density curves have the following properties:

The area under the curve always adds up to 100%.
The curve will never dip below the x-axis.

There are a few essential rules about density curves:

The area under a density curve represents probability.
The area under a density curve = 1.




Advantages:
Density curve do not consider outliers.
Desity curve works great for a large amount of data like 100,000.

Disadvantages:
For low amount of data say data of 50 individuals. 
If we draw a density curve over the histogram of this Our density curve will be inaccurate. Because there will be lot of missing gaps.

Type of Density Curves:
1. Uniform Distribution
2. Triangular Distribution
3. Normal Distribution : This is the most important density curve in statistics.




###Normal Distribution or Gaussian Distribution And
Emperical Formula:



Normal distribution has a single peak. hance called as unimodal. In a normal distribution, data is symmetrically distributed with no skew. When plotted on a graph, the data follows a bell shape, with most values clustering around a central region. Normal distributions are also called Gaussian distributions or bell curves because of their shape.



The population parameters mew and sigma are very important when we talk about normally distributed population. 
In normal distribution most of the data are located near the mean. 

Population mean or mew: Characterizes the "position" of the normal distribution.
Population standerd deviation or sigma: characterizes the "Spread" of the normal distribution. Larger the standerd deviation the more spread of the normal distribution will be.



Note: in Normal distribution the curve get Taller when spread decreases and get shorter and flatter when spread increases because normal distribution is a density curve. and the total area of the density curve must remain = 1 or 100% and its constant. So, the changes in the width of the curve must be compunsated with the change in the hight of the curve to maintaining of the area of 1.




Empirical rule
The empirical rule, or the 68-95-99.7 rule, tells you where most of your values lie in a normal distribution:

        Around 68% of values are within 1 standard deviation from the mean.
        Around 95% of values are within 2 standard deviations from the mean.
        Around 99.7% of values are within 3 standard deviations from the mean.


Example: Using the empirical rule in a normal distribution
You collect SAT scores from students in a new test preparation course. The data follows a normal distribution with a mean score (M) of 1150 and a standard deviation (SD) of 150.
Following the empirical rule:

Around 68% of scores are between 1,000 and 1,300, 1 standard deviation above and below the mean.
Around 95% of scores are between 850 and 1,450, 2 standard deviations above and below the mean.
Around 99.7% of scores are between 700 and 1,600, 3 standard deviations above and below the mean.



Data range	        Percentage of data in the range
x̅ − s,  x̅ + s	                  68%
x̅ − 2s, x̅ + 2s	              95%
x̅ − 3s, x̅ + 3s	              99.7%


The empirical rule is a quick way to get an overview of your data and check for any outliers or extreme values that don’t follow this pattern.







####Z-Scores, Standardization, and the Standard Normal Distribution:

    Standard Normal distribution : is the special type of normal distribution that has mean of zero(0) and the standard deviation(sigma) of 1.
also called the z-distribution.

it means standard Normal distribution always centered at zero and has interval that increase by 1.

    Z Score: Z-scores tell you how many standard deviations from the mean each value lies.
Z score is easily calculated by Z-Score table or Standard Normal Table.

                This z score tells us the total amount of area contained to the left side of area of any value z.


Standardization: Any type of normal distribution is converted into standard Normal distribution. And this conversion is known as Standardization.
Conversion is done by a special formula.

Z-score formula	Explanation
z = (X – μ) / σ

x = individual value
μ = mean
σ = standard deviation



        #Feature Scaling

Feature scaling is one of the most important data preprocessing step in machine learning. There are some feature scaling techniques such as Normalization and Standardization that are the most popular.

    Standardization vs normalization:

Normalization typically means rescales the values into a range of [0,1]. Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance).
Normalization is used when the data doesn't have Gaussian distribution whereas Standardization is used on data having Gaussian distribution.






    1. Normalization or Min-Max Scaling:
 is used to transform features to be on a similar scale. This scales the range to [0, 1] or sometimes [-1, 1].
    
                    X_new = (X - X_min)/(X_max - X_min)

Normalization is useful when there are no outliers as it cannot cope up with them. Usually, we would scale age and not incomes because only a few people have high incomes but the age is close to uniform.


    2. Standardization or Z-Score Normalization:
 is the transformation of features by subtracting from mean and dividing by standard deviation. This is often called as Z-score.

                    X_new = (X - mean)/Std

Standardization can be helpful in cases where the data follows a Gaussian distribution. Standardization does not get affected by outliers because there is no predefined range of transformed features.




##Central Limit Theorem:

 Even though the means are calculated using data from a uniform distribution or exponential distribution or any other distribution or Density Curves, "the means themselves are always normally distributed".


The central limit theorem states that whenever a random sample of size n is taken from any distribution with mean and variance, then the sample mean will be approximately normally distributed with mean and variance. The larger the value of the sample size, the better the approximation to the normal.

The sample size of 30 is considered sufficient to see the effect of the Central Limit Theorem.



##Chebyshev's Inequality:

As we know the 68-95-99.7 rule in normal distributions, what should we do in case we don't know the given distribution or if it's not normal distribution. We can use the Chebyshev's Inequality to calculate probability of observations lying near the mean of the distribution.

Chebyshev's inequality:
Chebyshev's inequality, makes a fairly broad but useful statement about data dispersion for almost any data distribution. This theorem states that no more than 1 / k^2 of the distribution's values will be more than k standard deviations away from the mean. Looked at another way, 1 - (1 / k^2) of the distribution's values will lie within k standard deviations of the mean.

Chebyshev's inequality, also known as Chebyshev's theorem, makes a fairly broad but useful statement about data dispersion for almost any data distribution. This theorem states that no more than 1 / k^2 of the distribution's values will be more than k standard deviations away from the mean. Looked at another way, 1 - (1 / k^2) of the distribution's values will lie within k standard deviations of the mean. 

Chebyshev's Inequality Formula
In order to investigate this theorem, let's first compare the calculations to the 68-95-99.7 rule of thumb for normal distributions. Since those numbers represent the data lying inside the bounds, we use Chebyshev's inequality for data inside the bounds:

Probability = 1 - (1 / k2)

Mathematically, values less than or equal to 1 are not valid for this computation. However, plugging in the k values for 2 and 3 is relatively simple:

P(k=2): 1 - (1 / 22) = 1 - 0.25 = 0.75 (75%)

P(k=3): 1 - (1 / 32) = 1 - 0.11 = 0.89 (89%)

In these cases, Chebyshev's inequality states that at least 75% of the data will fall within 2 standard deviations of the mean, and 89% of the data is expected to fall within 3 standard deviations of the mean. This is less precise than the 95% and 99.7% values that can be used for a known normal distribution. However, Chebyshev's inequality is true for all data distributions, not just a normal distribution.





Covarience: 
It is very important for data preprocessing and Data analysis.
    Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together

The formula is:
Cov(X,Y) = Σ E((X – μ) E(Y – ν)) / n-1 where:

X is a random variable
E(X) = μ is the expected value (the mean) of the random variable X and
E(Y) = ν is the expected value (the mean) of the random variable Y
n = the number of items in the data set.
Σ summation notation.

e.g., 
Calculate covariance for the following data set:
x: 2.1, 2.5, 3.6, 4.0 (mean = 3.1)
y: 8, 10, 12, 14 (mean = 11)

Substitute the values into the formula and solve:
Cov(X,Y) = ΣE((X-μ)(Y-ν)) / n-1
= (2.1-3.1)(8-11)+(2.5-3.1)(10-11)+(3.6-3.1)(12-11)+(4.0-3.1)(14-11) /(4-1)
= (-1)(-3) + (-0.6)(-1)+(.5)(1)+(0.9)(3) / 3
= 3 + 0.6 + .5 + 2.7 / 3
= 6.8/3
= 2.267

The result is positive, meaning that the variables are positively related.




###Pearson Correlation Coefficient:  


Correlation coefficients are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression.
In fact, when anyone refers to the correlation coefficient, they are usually talking about Pearson’s.


The Pearson correlation coefficient (r) is the most common way of measuring a linear correlation. It is a number between –1 and 1 that measures the strength and direction of the relationship between two variables.



Correlation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:

1 indicates a strong positive relationship.
-1 indicates a strong negative relationship.
A result of zero indicates no relationship at all.


A correlation coefficient of 1 means that for every positive increase in one variable, there is a positive increase of a fixed proportion in the other. For example, shoe sizes go up in (almost) perfect correlation with foot length.
A correlation coefficient of -1 means that for every positive increase in one variable, there is a negative decrease of a fixed proportion in the other. For example, the amount of gas in a tank decreases in (almost) perfect correlation with speed.
Zero means that for every increase, there isn’t a positive or negative increase. The two just aren’t related.




QQ plot Check data is Normally Distributed: 

Main motive of this QQ plot is to know wheather the given data is Normally Distributed or not.

A Q-Q plot, short for “quantile-quantile” plot, is used to assess whether or not a set of data potentially came from some theoretical distribution.
In most cases, this type of plot is used to determine whether or not a set of data follows a normal distribution.

If the data is normally distributed, the points in a Q-Q plot will lie on a straight diagonal line.

Conversely, the more the points in the plot deviate significantly from a straight diagonal line, the less likely the set of data follows a normal distribution.


    Limitation of QQ plot: 
If number of given sample(data) is small then it is hard to interpret QQ plot.




###Bernoulli Distribution:

Any event where we only have 1 Trial and 2 Possible outcomes follows this Bernoulli Distribution.
e.g., coin flip.


##Binomial Distribution:

A sequence of identical Bernoulli events, 


quiz consist of 10 true and false questions. 

guessing 1 question is a         =      Bernoulli event,
And guessing the entire quiz is a =      Binomia event


Bernoulli event: which outcome we expect for a single trial.

Binomia event: the number of times we expect to get a specific outcome.



If a random variable X follows a Binomial distribution, then the probability that X = k successes can be found by the following formula:

P(X=k) = nCk * pk * (1-p)n-k

where:

n: number of trials
k: number of successes
p: probability of success on a given trial
nCk: the number of ways to obtain k successes in n trials
For example, suppose we flip a coin 3 times. We can use the formula above to determine the probability of obtaining 0 heads during these 3 flips:

P(X=0) = 3C0 * .50 * (1-.5)3-0 = 1 * 1 * (.5)3 = 0.125


When n = 1 trial, the Binomial distribution is equivalent to the Bernoulli distribution.




    Log Normal Distribution:

The log-normal distribution is a right skewed continuous distribution of a random variable,  meaning it has a long tail towards the right. 
Thus, if the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution.
It is used for modelling various natural phenomena such as income distributions, the length of chess games or the time to repair a maintainable system and more.


Right skewed distribution also called Positively Skewed Distribution.

Positive Skewness means when the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode.


So, when is the skewness too much?
The rule of thumb seems to be:

If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.
If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.
If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.


property of positively skewed distribution:  Mean  >  Median  >  Mode



If you get a right skewed continuous distribution(log normal dataset) then by applying a log function to your log-normal dataset, your dataset will become normally distributed.
means that when you take the log of your log-normal data you end up with a normal distribution.




e.g., wealth distribution, 
If we plot an histogram of incomes of people according to us dollar then the plot will be right skewed because there are some people income which goes to 1 billion, 2 billion , 3 billion etc. but that type of peoples are very less. 
So, After applying log function the dataset is called log normally distributed.




Kurtosis
Kurtosis is all about the tails of the distribution — not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution.

High kurtosis in a data set is an indicator that data has heavy tails or outliers. If there is a high kurtosis, then, we need to investigate why do we have so many outliers. It indicates a lot of things, maybe wrong data entry or other things. Investigate!
Low kurtosis in a data set is an indicator that data has light tails or lack of outliers. If we get low kurtosis(too good to be true), then also we need to investigate and trim the dataset of unwanted results.



    Why do we need it:

In machine learning,  simple Leanear Regression works very efficiently with normal distribution, instead of right skewed or left skewed curve. 
So, thats why we should and we have to convert the right skewed curve into normal distribution.
This process is called data Transformation technique.



##Power Law Distribution:

follows 80 20 principle. pareto principle

Just like log normal distribution, we can also convert power law distribution into normal distribution.

We can transform it with box cox transformation.


The power law (also called the scaling law) states that a relative change in one quantity results in a proportional relative change in another. T

it implies a small amount of occurrences is common, while larger occurrences are rare.

For example, where the distribution of income is concerned, there are very few billionaires; the bulk of the population holds very modest nest eggs.


Other examples of phenomena with this type of distribution:
    Distribution of income,
    Magnitude of earthquakes,
    Size of cities according to population,
    Size of corporations,
    Trading volumes on the stock market,
    word frequencies.




###Box-Cox Transformation:

Box-Cox transformation transform pareto distribution into normal distribution.




###Confidence Interval In statistics:

    What Is Confidence Interval?

    What is cocnfidence:
Confidence, in statistics, is another way to describe probability. For example, if you construct a confidence interval with a 95% confidence level, you are confident that 95 out of 100 times the estimate will fall between the upper and lower values specified by the confidence interval.


What exactly is a confidence interval?
A confidence interval is the mean of your estimate plus and minus the variation in that estimate. This is the range of values you expect your estimate to fall between if you redo your test, within a certain level of confidence.


A 95% confidence interval is just an interval that covers 95% of the means. after bootstrapping the sample.


A confidence interval, in statistics, refers to the probability that a population parameter will fall between a set of values for a certain proportion of times.

    What Does a Confidence Interval Reveal?
A confidence interval is a range of values, bounded above and below the statistic's mean, that likely would contain an unknown population parameter. Confidence level refers to the percentage of probability, or certainty, that the confidence interval would contain the true population parameter when you draw a random sample many times.


The confidence interval is the range of values that you expect your estimate to fall between a certain percentage of the time if you run your experiment again or re-sample the population in the same way.

The confidence level is the percentage of times you expect to reproduce an estimate between the upper and lower bounds of the confidence interval, and is set by the alpha value.

    How Are Confidence Intervals Used?
Statisticians use confidence intervals to measure uncertainty in a sample variable. For example, a researcher selects different samples randomly from the same population and computes a confidence interval for each sample to see how it may represent the true value of the population variable. The resulting datasets are all different where some intervals include the true population parameter and others do not.


    What Is a T-Test?
Confidence intervals are conducted using statistical methods, such as a t-test. A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related to certain features. Calculating a t-test requires three key data values. They include the difference between the mean values from each data set (called the mean difference), the standard deviation of each group, and the number of data values of each group.





##Type I error and Type II error:

Type  1 error is when you 'reject' a "true" null hypothesis. also called false positive.
The probability of making this error is denoted by alpha(α)



Type 2 error is when you 'accept' a "false" null hypothisis. also called false negative.
The probability of making this error is denoted by beta(β)

e.g., 
Type I error (false positive): the test result says you have coronavirus, but you actually don’t.
Type II error (false negative): the test result says you don’t have coronavirus, but you actually do.







####One Tailed And 2 Tailed Tests:

     ## two-tailed test

Two-tailed hypothesis tests are also known as nondirectional and two-sided tests because you can test for effects in both directions. When you perform a two-tailed test, you split the significance level percentage between both tails of the distribution. In the example below, I use an alpha of 5% and the distribution has two shaded regions of 2.5% (2 * 2.5% = 5%).

In a two-tailed test, the generic null and alternative hypotheses are the following:

Null: The effect equals zero.
Alternative:  The effect does not equal zero.


Some Technical Stuff:
    Alpha levels:

Alpha levels (sometimes just called “significance levels”) are used in hypothesis tests; it is the probability of making the wrong decision when the null hypothesis is true. A one-tailed test has the entire 5% of the alpha level in one tail (in either the left, or the right tail). A two-tailed test splits your alpha level in half (as in the image to the left).

Let’s say you’re working with the standard alpha level of 0.5 (5%). A two tailed test will have half of this (2.5%) in each tail. Very simply, the hypothesis test might go like this:

If this test statistic falls in the top 2.5% or bottom 2.5% of its probability distribution (in this case, the t-distribution), you would reject the null hypothesis.



The “cut off” areas created by your alpha levels are called rejection regions. It’s where you would reject the null hypothesis, if your test statistic happens to fall into one of those rejection areas. The terms “one tailed” and “two tailed” can more precisely be defined as referring to where your rejection regions are located.



One-Tailed Hypothesis Tests
One-tailed hypothesis tests are also known as directional and one-sided tests because you can test for effects in only one direction. When you perform a one-tailed test, the entire significance level percentage goes into the extreme end of one tail of the distribution.

In the examples below, I use an alpha of 5%. Each distribution has one shaded region of 5%. When you perform a one-tailed test, you must determine whether the critical region is in the left tail or the right tail. The test can detect an effect only in the direction that has the critical region. It has absolutely no capacity to detect an effect in the other direction.

In a one-tailed test, you have two options for the null and alternative hypotheses, which corresponds to where you place the critical region.

You can choose either of the following sets of generic hypotheses:

Right Tailed:

Null: The effect is less than or equal to zero.
Alternative: The effect is greater than zero.


Or:

Left Tailed:

Null: The effect is greater than or equal to zero.
Alternative: The effect is less than zero.




Example question #1: A government official claims that the dropout rate for local schools is 25%. Last year, 190 out of 603 students dropped out. Is there enough evidence to reject the government official’s claim?

Example question #2: A government official claims that the dropout rate for local schools is less than 25%. Last year, 190 out of 603 students dropped out. Is there enough evidence to reject the government official’s claim?

Example question #3: A government official claims that the dropout rate for local schools is greater than 25%. Last year, 190 out of 603 students dropped out. Is there enough evidence to reject the government official’s claim?

Step 1: Read the question.

Step 2: Rephrase the claim in the question with an equation.

In example question #1, Drop out rate = 25%
In example question #2, Drop out rate < 25%
In example question #3, Drop out rate > 25%.

Step 3: If step 2 has an equals sign in it, this is a two-tailed test. If it has > or < it is a one-tailed test.




####Hypothesis Testing:

The main purpose of statistics is to test a hypothesis. For example, you might run an experiment and find that a certain drug is effective at treating headaches. But if you can’t repeat that experiment, no one will take your results seriously. 

Hypothesis testing in statistics is a way for you to test the results of a survey or experiment to see if you have meaningful results. You’re basically testing whether your results are valid by figuring out the odds that your results have happened by chance. If your results may have happened by chance, the experiment won’t be repeatable and so has little use.


The Null Hypothesis is the assumption that the event will not occur. A null hypothesis has no bearing on the study's outcome unless it is rejected.

but this Assumption believed to be true.

Innocence = Null. Presumed innocence, Rejected only by strong evidence 

H0 is the symbol for it, and it is pronounced H-naught.

The Alternate Hypothesis is the logical opposite of the null hypothesis. The acceptance of the alternative hypothesis follows the rejection of the null hypothesis. H1 is the symbol for it.



Let's understand this with an example.

A sanitizer manufacturer claims that its product kills 95 percent of germs on average. 

To put this company's claim to the test, create a null and alternate hypothesis.

H0 (Null Hypothesis): Average = 95%.

Alternative Hypothesis (H1): The average is less than 95%.





from StatQuest: """""""""""We can create a hypothesis and if data gives us strong evidence that the hypothesis is wrong then we can "reject the hypothesis" but when we have data that is similar to the hypothesis but not exactly the same then the best we can do is "fail to reject the hypothesis"."""""""""""""


The hypothesis that there is no difference between things is called the "null hypothesis".

 

the goal of collecting all of this data is to determine if we should reject or fail to reject the null hypothesis in order to decide if we should reject or fail to reject the null hypothesis.

we run the data through something called a "statistical test" and the output of the statistical test is a decision to reject or fail to reject the null hypothesis 


A statistical test needs three things: 

    1. it needs data

    2. it needs a null or primary hypothesis:
    i.e.  it needs something to reject or fail to reject and three it needs an

    3. alternative hypothesis:
    in this case thealternative hypothesis is simply the opposite of the null hypothesis






P-Value:

It is the probability for the null hypothesis to be true.

Null Hypothesis: It treats everything same or equal.


        Immeture Thoughts: 
                    It is to know what is the percentage of accepting the null hypothesis. means what is the percentage to tell drug a and drug b are the same.

Let The threshold is 5%. means if the p-value is 5% or more than 5%, we will say that we accept null hypothesis. we fail to reject null hypothesis.
means Drug A and drug B are same, they are not different.

But 
if the p-value is less than 5%, we are confident enough to reject the null hypothesis.
means, we are rejecting the fact that the drug A and B are same. It means we confident enough to say that the Drug A and Drug B are Different from each other. 
the closer a p-value is to zero the more confidence we have that drug a and drug b are different.



        Real Thing Starts here:

The p-value helps us decide if we should reject the Null Hypothesis or not.


                            p-values are numbers between 0 and 1 that in this example quantify how confident we should be that drug a is different from drug b.
the closer a p-value is to zero the more confidence we have that drug a and drug b are different.

So, the question here is 
"how small does a p-value have to be before we are sufficienty confident that Drug A is different from Drug B"?
means what is the threshold can we use to make a good decision?

"In practice a commonly used threshold is 0.05."

It means that we are 5% confident to reject the null hypothesis. and 5% confident to accept the alternative hypothesis.

In this case null Hypothesis is that the Drugs are the same. and the Alternative Hypothesis is that the Drugs are Different from each other.

If The p-value is less than 0.05, then we will decide that Drug A is different from Drug B.



Why we took threshold as 5%:

Because when there is no difference between Drug A and Drug B, even then also, After doing experiment 5% time we get p-vlaue. which should not have to come and we don't want to get p-value when there is no difference. But unfortunately we are getting.
but then The value will called a "False Positive". But the good thing is that the value will always be less then 0.05.


We could decide some other threshold based on the type of experiment and importance of the experiment. like 0.2, 0.00001 etc.

like we are trying to decide if the ice-cream truck will arrive on time. then we can use a larger threshold, like 0.2
Means, we are willing to get a False Positive 2 times out of 10.

But 

The most common Threshold is 0.05.


Note: A small p-Value helps us to if Drug A is different form Drug B, it does not tell us "how different they are".
The differene can be tiny or huge.




###Steps in Hypothesis Testing:


    Step 1: State your null and alternate hypothesis:



    Step 2:Decide on the significance level:
The second step is to specify the α level which is also known as the significance level. Typical values are 0.05 and 0.01.



    Step 3: Collect data
Remember the importance of recognizing whether data is collected through an experimental design or observational study.


    Step 4: Perform a statistical test

    Step 5: Decide whether to reject or fail to reject your null hypothesis
Based on the outcome of your statistical test, you will have to decide whether to reject or fail to reject your null hypothesis.

In most cases you will use the p-value generated by your statistical test to guide your decision. And in most cases, your predetermined level of significance for rejecting the null hypothesis will be 0.05 – that is, when there is a less than 5% chance that you would see these results if the null hypothesis were true.



    Step 6: Present your findings:
Once we have found the p-value or rejection region, and made a statistical decision about the null hypothesis (i.e. we will reject the null or fail to reject the null), we then want to summarize our results into an overall conclusion for our test.







##statistical tests:


    Chi-Square Test:
Chi-Square test is used when we perform hypothesis testing on two categorical variables from a single population or we can say that to compare categorical variables from a single population.
Use Chi-Square Tests when every variable you’re working with is categorical.




    T-Test:

 A t-test is used when the population parameters (mean and standard deviation) are not known.

The T-test is an inferential statistic that is used to determine the difference or to compare the means of two groups of samples which may be related to certain features. It is performed on continuous variables.

There are three different versions of t-tests:

→ One sample t-test which tells whether means of sample and population are different.

→ Two sample t-test also is known as Independent t-test — it compares the means of two independent groups and determines whether there is statistical evidence that the associated population means are significantly different.

→ Paired t-test when you want to compare means of the different samples from the same group or which compares means from the same group at different times.




    ANOVA Test:
It is also called an analysis of variance and is used to compare multiple (three or more) samples with a single test. It is used when the categorical feature has more than two categories.
Use ANOVA when you have at least one categorical variable and one continuous dependent variable.



